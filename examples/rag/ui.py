import streamlit as st
import json
from datetime import datetime
from docx import Document
from typing import Callable, Generator, Tuple

from db import reset_db, insert_text, query_text
from llm import ask_llm

USE_STREAM = True

st.set_page_config(
    page_title="My RAG",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Init session
if "user" not in st.session_state:
    st.session_state.update({
        "user": None,
        "chat_history": [],
        "current_db": "default"
    })

def read_file(uploaded_file):
    content = ""
    try:
        if uploaded_file.type in ["text/plain", "text/x-markdown"]:      
            content = uploaded_file.read().decode("utf-8")
        elif uploaded_file.type == "application/pdf":
            import PyPDF2
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            content = "\n".join([page.extract_text() for page in pdf_reader.pages])
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            doc = Document(uploaded_file)
            content = "\n".join([para.text for para in doc.paragraphs])
        else:
            raise ValueError(f"Unsupported file types: {uploaded_file.type}")
    except Exception as e:
        raise RuntimeError(f"Fail to read files: {str(e)}. Type: {uploaded_file.type}")
    return content


def enhanced_file_processor(file):
    content = read_file(file)
    content = content.replace("\n", " ") 
    content = " ".join(content.split())
    return content

def context_aware_query(collection_name, query, history, llm_url, llm_api_key, llm_model, embed_model_path, is_stream = True) -> Tuple[str | Callable[[], Generator[str, None, None]], str]:
    context = "\n".join([f"Q: {q}\nA: {a}" for q, a, d in history[-3:]])
    enhanced_query = f"{context}\n\nNew questionÔºö{query}"
    
    retrieved_docs = query_text(
        collection_name=collection_name,
        query=enhanced_query,
        embed_model_path=embed_model_path,
    )
    return ask_llm(llm_url, llm_api_key, llm_model, query, retrieved_docs, is_stream), retrieved_docs

def main_interface():
    with st.sidebar:
        with st.expander("üõ†Ô∏è Service Setting", expanded=True):
            llm_url = st.text_input("LLM Base URL", value="https://api.lkeap.cloud.tencent.com/v1/chat")
            llm_api_key = st.text_input("LLM API Key", value="Your API Key")
            llm_model = st.text_input("LLM Model", value="deepseek-v3")
            embed_model_path = st.text_input("Embedding Model", value="BAAI/bge-small-zh-v1.5")

        with st.expander("üìö Knowledge Base Management", expanded=True):
            with st.form("upload_form"):
                collection_name = 'rag_collection'
                uploaded_file = st.file_uploader(
                    "Upload documents", 
                    type=["txt", "pdf", "docx", "md"],
                    accept_multiple_files=True
                )
                if st.form_submit_button("üöÄ Start processing"):
                    reset_db()
                    if uploaded_file:
                        success = True
                        for file in uploaded_file:
                            with st.spinner(f"Process {file.name}..."):
                                content = enhanced_file_processor(file)
                                success = insert_text(
                                    collection_name=collection_name,
                                    docs=content,
                                    embed_model_path=embed_model_path,
                                    chunksize=256,
                                    overlap=25
                                )
                                if not success:
                                    break
                        if success:
                            st.success("Document processing completed")
                        else:
                            st.error("Document processing failed!")
                    else:
                        st.error("No document uploaded yet!")

        with st.expander("üóÉÔ∏è Dialogue management", expanded=True):
            if st.button("üîÑ Clear all records", 
                        use_container_width=True):
                st.session_state.chat_history = []
                st.rerun()
                
            if st.download_button("üíæ Export all records",
                data=json.dumps(st.session_state.chat_history),
                file_name=f"chat_{datetime.now().strftime('%Y%m%d')}.json",
                mime="application/json",
                use_container_width=True):
                st.toast("Exported!")

    with st.container():
        st.header("üí¨ RAG QA")
        
        # Real-time dialog display container
        chat_container = st.container(height=600, border=False)
        # Render the existing history first
        with chat_container:
            for idx, (q, a, d) in enumerate(st.session_state.chat_history):
                with st.chat_message("user", avatar="üë§"):
                    st.markdown(f"{q}")
                with st.chat_message("assistant", avatar="ü§ñ"):
                    st.markdown(a)
                    if d:
                        with st.popover("References"):
                            st.markdown(d)

        # Input processing
        if prompt := st.chat_input("Please enter your question..."):
            # Display user input
            with chat_container:
                with st.chat_message("user", avatar="üë§"):
                    st.markdown(f"{prompt}")
                
                # Add a placeholder response
                with st.chat_message("assistant", avatar="ü§ñ"):
                    answer_placeholder = st.empty()
                    answer_placeholder.markdown("‚ñå")

            # Execute the query (keep the original logic)
            try:
                resp_or_stream, retrieved_docs = context_aware_query(
                    collection_name=collection_name,
                    query=prompt,
                    history=st.session_state.chat_history,
                    llm_url=llm_url,
                    llm_api_key=llm_api_key,
                    llm_model=llm_model,
                    embed_model_path=embed_model_path,
                    is_stream=USE_STREAM,
                )
                
                # Update the answer
                full_resp = ""
                if USE_STREAM:
                    for chunk in resp_or_stream():
                        full_resp += chunk
                        answer_placeholder.markdown(full_resp)
                else:
                    full_resp = resp_or_stream
                    answer_placeholder.markdown(full_resp)
                
                # Update session history
                st.session_state.chat_history.append(
                    (prompt, full_resp, retrieved_docs)
                )
                
            except Exception as e:
                answer_placeholder.error(f"Handling failures: {str(e)}")
                st.session_state.chat_history.append(
                    (prompt, f"‚ö†Ô∏è Error: {str(e)}", "")
                )

            st.rerun()


def main():
    main_interface()


if __name__ == "__main__":
    main()
